{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-gatO9aZrxT",
        "outputId": "83551c90-75b7-44b8-e0a3-ce38617530d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AdamW\n",
        "\n",
        "teacher_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "teacher_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "student_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "student_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "student_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "teacher_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "def distillation_loss(teacher_logits, student_logits):\n",
        "    return nn.KLDivLoss()(nn.functional.log_softmax(student_logits, dim=1), nn.functional.softmax(teacher_logits, dim=1))\n",
        "\n",
        "batch_size = 4\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 5\n",
        "\n",
        "input_data = [\"The Industrial Revolution was a period of significant economic, technological, and social change that began in the late 18th century and continued into the 19th century. It marked a shift from agrarian and handicraft-based economies to industrial and machine-based economies. This period saw the rapid development of factories, mechanized agriculture, and the use of steam power. It had a profound impact on society, leading to urbanization, changes in labor practices, and increased production. The Industrial Revolution is often considered a turning point in history.\"]\n",
        "labels = [\"The Industrial Revolution, which started in the late 18th century, brought about significant economic, technological, and social changes. It led to the rise of industrial economies, the use of machinery, and urbanization.\"]\n",
        "\n",
        "input_ids = student_tokenizer.encode(input_data[0], return_tensors=\"pt\", max_length=50, padding=\"max_length\", truncation=True)\n",
        "label_ids = student_tokenizer.encode(labels[0], return_tensors=\"pt\", max_length=50, padding=\"max_length\", truncation=True)\n",
        "\n",
        "dataset = TensorDataset(input_ids, label_ids)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(student_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student_model.train()\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids_batch, label_ids_batch = batch\n",
        "        student_logits = student_model(input_ids_batch).logits\n",
        "        teacher_logits = teacher_model(input_ids_batch).logits\n",
        "\n",
        "        loss = distillation_loss(teacher_logits, student_logits)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "student_model.save_pretrained(\"student_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLGgwLz_ZzdC",
        "outputId": "fc5e2ff8-b7c5-4fa4-bc1f-417567899e37"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Once upon a time, in a land far, far away, \"\n",
        "input_ids = student_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "generated_text = student_model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
        "generated_text = student_tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwSUj43yce_O",
        "outputId": "131edf2e-f976-4877-ca04-2c13dbe7f9db"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a land far, far away, Â I was in the middle of a great deal of trouble. I was a little bit of an idiot.\n",
            "I had a lot of problems with my life. And I had problems. But I didn't have a problem with the world. It was all about the things that I wanted to do. So I just wanted it to be all right. That was the way I got it. The way. You know, I\n"
          ]
        }
      ]
    }
  ]
}